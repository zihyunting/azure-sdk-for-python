# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import json
import sys
from typing import Any, Callable, Dict, IO, List, Optional, TypeVar, Union, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.rest import HttpRequest, HttpResponse
from azure.core.tracing.decorator import distributed_trace
from azure.core.utils import case_insensitive_dict

from .. import models as _models
from .._model_base import SdkJSONEncoder, _deserialize
from .._serialization import Serializer
from .._vendor import ModelClientMixinABC

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
_Unset: Any = object()
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]

_SERIALIZER = Serializer()
_SERIALIZER.client_side_validation = False


def build_model_get_chat_completions_request(
    *,
    unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
    model_deployment: Optional[str] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2024-04-01-preview"))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/v1/chat/completions"

    # Construct parameters
    _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")

    # Construct headers
    if unknown_parameters is not None:
        _headers["unknown-parameters"] = _SERIALIZER.header("unknown_parameters", unknown_parameters, "str")
    if model_deployment is not None:
        _headers["azureml-model-deployment"] = _SERIALIZER.header("model_deployment", model_deployment, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


def build_model_get_embeddings_request(
    *,
    unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
    model_deployment: Optional[str] = None,
    **kwargs: Any
) -> HttpRequest:
    _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
    _params = case_insensitive_dict(kwargs.pop("params", {}) or {})

    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2024-04-01-preview"))
    accept = _headers.pop("Accept", "application/json")

    # Construct URL
    _url = "/v1/embeddings"

    # Construct parameters
    _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")

    # Construct headers
    if unknown_parameters is not None:
        _headers["unknown-parameters"] = _SERIALIZER.header("unknown_parameters", unknown_parameters, "str")
    if model_deployment is not None:
        _headers["azureml-model-deployment"] = _SERIALIZER.header("model_deployment", model_deployment, "str")
    _headers["Accept"] = _SERIALIZER.header("accept", accept, "str")
    if content_type is not None:
        _headers["Content-Type"] = _SERIALIZER.header("content_type", content_type, "str")

    return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)


class ModelClientOperationsMixin(ModelClientMixinABC):
    @overload
    def get_chat_completions(
        self,
        body: JSON,
        *,
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param body: Required.
        :type body: JSON
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # The input is polymorphic. The following are possible polymorphic inputs based off
                  discriminator "type":

                # JSON input template for discriminator value "json_object":
                chat_completions_response_format = {
                    "type": "json_object"
                }

                # JSON input template for discriminator value "text":
                chat_completions_response_format = {
                    "type": "text"
                }

                # JSON input template you can fill out and use as your body input.
                body = {
                    "messages": [
                        chat_request_message
                    ],
                    "frequency_penalty": 0.0,  # Optional. A value that influences the
                      probability of generated tokens appearing based on their cumulative frequency in
                      generated text. Positive values will make tokens less likely to appear as their
                      frequency increases and decrease the likelihood of the model repeating the same
                      statements verbatim.
                    "max_tokens": 0,  # Optional. The maximum number of tokens to generate.
                    "presence_penalty": 0.0,  # Optional. A value that influences the probability
                      of generated tokens appearing based on their existing presence in generated text.
                      Positive values will make tokens less likely to appear when they already exist
                      and increase the model's likelihood to output new topics.
                    "response_format": chat_completions_response_format,
                    "seed": 0,  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed, and you
                      should refer to the system_fingerprint response parameter to monitor changes in
                      the backend.".
                    "stop": [
                        "str"  # Optional. A collection of textual sequences that will end
                          completions generation.
                    ],
                    "stream": bool,  # Optional. A value indicating whether chat completions
                      should be streamed for this request.
                    "temperature": 0.0,  # Optional. The sampling temperature to use that
                      controls the apparent creativity of generated completions. Higher values will
                      make output more random while lower values will make results more focused and
                      deterministic. It is not recommended to modify temperature and top_p for the same
                      completions request as the interaction of these two settings is difficult to
                      predict.
                    "tool_choice": "str",  # Optional. If specified, the model will configure
                      which of the provided tools it can use for the chat completions response. Is
                      either a Union[str, "_models.ChatCompletionsToolSelectionPreset"] type or a
                      ChatCompletionsNamedToolSelection type.
                    "tools": [
                        chat_completions_tool_definition
                    ],
                    "top_p": 0.0  # Optional. An alternative to sampling with temperature called
                      nucleus sampling. This value causes the model to consider the results of tokens
                      with the provided probability mass. As an example, a value of 0.15 will cause
                      only the tokens comprising the top 15% of probability mass to be considered. It
                      is not recommended to modify temperature and top_p for the same completions
                      request as the interaction of these two settings is difficult to predict.
                }

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @overload
    def get_chat_completions(
        self,
        *,
        messages: List[_models.ChatRequestMessage],
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        content_type: str = "application/json",
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        response_format: Optional[_models.ChatCompletionsResponseFormat] = None,
        stop: Optional[List[str]] = None,
        stream_parameter: Optional[bool] = None,
        tools: Optional[List[_models.ChatCompletionsToolDefinition]] = None,
        tool_choice: Optional[
            Union[str, _models.ChatCompletionsToolSelectionPreset, _models.ChatCompletionsNamedToolSelection]
        ] = None,
        seed: Optional[int] = None,
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :keyword messages: The collection of context messages associated with this chat completions
         request.
         Typical usage begins with a chat message for the System role that provides instructions for
         the behavior of the assistant, followed by alternating messages between the User and
         Assistant roles. Required.
        :paramtype messages: list[~azure.ai.inference.models.ChatRequestMessage]
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword frequency_penalty: A value that influences the probability of generated tokens
         appearing based on their cumulative
         frequency in generated text.
         Positive values will make tokens less likely to appear as their frequency increases and
         decrease the likelihood of the model repeating the same statements verbatim. Default value is
         None.
        :paramtype frequency_penalty: float
        :keyword presence_penalty: A value that influences the probability of generated tokens
         appearing based on their existing
         presence in generated text.
         Positive values will make tokens less likely to appear when they already exist and increase
         the
         model's likelihood to output new topics. Default value is None.
        :paramtype presence_penalty: float
        :keyword temperature: The sampling temperature to use that controls the apparent creativity of
         generated completions.
         Higher values will make output more random while lower values will make results more focused
         and deterministic.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature called nucleus sampling. This value
         causes the
         model to consider the results of tokens with the provided probability mass. As an example, a
         value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
         considered.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype top_p: float
        :keyword max_tokens: The maximum number of tokens to generate. Default value is None.
        :paramtype max_tokens: int
        :keyword response_format: An object specifying the format that the model must output. Used to
         enable JSON mode. Default value is None.
        :paramtype response_format: ~azure.ai.inference.models.ChatCompletionsResponseFormat
        :keyword stop: A collection of textual sequences that will end completions generation. Default
         value is None.
        :paramtype stop: list[str]
        :keyword stream_parameter: A value indicating whether chat completions should be streamed for
         this request. Default value is None.
        :paramtype stream_parameter: bool
        :keyword tools: The available tool definitions that the chat completions request can use,
         including caller-defined functions. Default value is None.
        :paramtype tools: list[~azure.ai.inference.models.ChatCompletionsToolDefinition]
        :keyword tool_choice: If specified, the model will configure which of the provided tools it can
         use for the chat completions response. Is either a Union[str,
         "_models.ChatCompletionsToolSelectionPreset"] type or a ChatCompletionsNamedToolSelection type.
         Default value is None.
        :paramtype tool_choice: str or ~azure.ai.inference.models.ChatCompletionsToolSelectionPreset or
         ~azure.ai.inference.models.ChatCompletionsNamedToolSelection
        :keyword seed: If specified, the system will make a best effort to sample deterministically
         such that repeated requests with the
         same seed and parameters should return the same result. Determinism is not guaranteed, and you
         should refer to the
         system_fingerprint response parameter to monitor changes in the backend.". Default value is
         None.
        :paramtype seed: int
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @overload
    def get_chat_completions(
        self,
        body: IO[bytes],
        *,
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param body: Required.
        :type body: IO[bytes]
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @distributed_trace
    def get_chat_completions(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        messages: List[_models.ChatRequestMessage] = _Unset,
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        response_format: Optional[_models.ChatCompletionsResponseFormat] = None,
        stop: Optional[List[str]] = None,
        stream_parameter: Optional[bool] = None,
        tools: Optional[List[_models.ChatCompletionsToolDefinition]] = None,
        tool_choice: Optional[
            Union[str, _models.ChatCompletionsToolSelectionPreset, _models.ChatCompletionsNamedToolSelection]
        ] = None,
        seed: Optional[int] = None,
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword messages: The collection of context messages associated with this chat completions
         request.
         Typical usage begins with a chat message for the System role that provides instructions for
         the behavior of the assistant, followed by alternating messages between the User and
         Assistant roles. Required.
        :paramtype messages: list[~azure.ai.inference.models.ChatRequestMessage]
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword frequency_penalty: A value that influences the probability of generated tokens
         appearing based on their cumulative
         frequency in generated text.
         Positive values will make tokens less likely to appear as their frequency increases and
         decrease the likelihood of the model repeating the same statements verbatim. Default value is
         None.
        :paramtype frequency_penalty: float
        :keyword presence_penalty: A value that influences the probability of generated tokens
         appearing based on their existing
         presence in generated text.
         Positive values will make tokens less likely to appear when they already exist and increase
         the
         model's likelihood to output new topics. Default value is None.
        :paramtype presence_penalty: float
        :keyword temperature: The sampling temperature to use that controls the apparent creativity of
         generated completions.
         Higher values will make output more random while lower values will make results more focused
         and deterministic.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature called nucleus sampling. This value
         causes the
         model to consider the results of tokens with the provided probability mass. As an example, a
         value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
         considered.
         It is not recommended to modify temperature and top_p for the same completions request as the
         interaction of these two settings is difficult to predict. Default value is None.
        :paramtype top_p: float
        :keyword max_tokens: The maximum number of tokens to generate. Default value is None.
        :paramtype max_tokens: int
        :keyword response_format: An object specifying the format that the model must output. Used to
         enable JSON mode. Default value is None.
        :paramtype response_format: ~azure.ai.inference.models.ChatCompletionsResponseFormat
        :keyword stop: A collection of textual sequences that will end completions generation. Default
         value is None.
        :paramtype stop: list[str]
        :keyword stream_parameter: A value indicating whether chat completions should be streamed for
         this request. Default value is None.
        :paramtype stream_parameter: bool
        :keyword tools: The available tool definitions that the chat completions request can use,
         including caller-defined functions. Default value is None.
        :paramtype tools: list[~azure.ai.inference.models.ChatCompletionsToolDefinition]
        :keyword tool_choice: If specified, the model will configure which of the provided tools it can
         use for the chat completions response. Is either a Union[str,
         "_models.ChatCompletionsToolSelectionPreset"] type or a ChatCompletionsNamedToolSelection type.
         Default value is None.
        :paramtype tool_choice: str or ~azure.ai.inference.models.ChatCompletionsToolSelectionPreset or
         ~azure.ai.inference.models.ChatCompletionsNamedToolSelection
        :keyword seed: If specified, the system will make a best effort to sample deterministically
         such that repeated requests with the
         same seed and parameters should return the same result. Determinism is not guaranteed, and you
         should refer to the
         system_fingerprint response parameter to monitor changes in the backend.". Default value is
         None.
        :paramtype seed: int
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # The input is polymorphic. The following are possible polymorphic inputs based off
                  discriminator "type":

                # JSON input template for discriminator value "json_object":
                chat_completions_response_format = {
                    "type": "json_object"
                }

                # JSON input template for discriminator value "text":
                chat_completions_response_format = {
                    "type": "text"
                }

                # JSON input template you can fill out and use as your body input.
                body = {
                    "messages": [
                        chat_request_message
                    ],
                    "frequency_penalty": 0.0,  # Optional. A value that influences the
                      probability of generated tokens appearing based on their cumulative frequency in
                      generated text. Positive values will make tokens less likely to appear as their
                      frequency increases and decrease the likelihood of the model repeating the same
                      statements verbatim.
                    "max_tokens": 0,  # Optional. The maximum number of tokens to generate.
                    "presence_penalty": 0.0,  # Optional. A value that influences the probability
                      of generated tokens appearing based on their existing presence in generated text.
                      Positive values will make tokens less likely to appear when they already exist
                      and increase the model's likelihood to output new topics.
                    "response_format": chat_completions_response_format,
                    "seed": 0,  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed, and you
                      should refer to the system_fingerprint response parameter to monitor changes in
                      the backend.".
                    "stop": [
                        "str"  # Optional. A collection of textual sequences that will end
                          completions generation.
                    ],
                    "stream": bool,  # Optional. A value indicating whether chat completions
                      should be streamed for this request.
                    "temperature": 0.0,  # Optional. The sampling temperature to use that
                      controls the apparent creativity of generated completions. Higher values will
                      make output more random while lower values will make results more focused and
                      deterministic. It is not recommended to modify temperature and top_p for the same
                      completions request as the interaction of these two settings is difficult to
                      predict.
                    "tool_choice": "str",  # Optional. If specified, the model will configure
                      which of the provided tools it can use for the chat completions response. Is
                      either a Union[str, "_models.ChatCompletionsToolSelectionPreset"] type or a
                      ChatCompletionsNamedToolSelection type.
                    "tools": [
                        chat_completions_tool_definition
                    ],
                    "top_p": 0.0  # Optional. An alternative to sampling with temperature called
                      nucleus sampling. This value causes the model to consider the results of tokens
                      with the provided probability mass. As an example, a value of 0.15 will cause
                      only the tokens comprising the top 15% of probability mass to be considered. It
                      is not recommended to modify temperature and top_p for the same completions
                      request as the interaction of these two settings is difficult to predict.
                }

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str",  # The chat role associated with the
                                  message. Required. Known values are: "system", "user", "assistant",
                                  and "tool".
                                "tool_calls": [
                                    chat_completions_tool_call
                                ]
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.ChatCompletions] = kwargs.pop("cls", None)

        if body is _Unset:
            if messages is _Unset:
                raise TypeError("missing required argument: messages")
            body = {
                "frequency_penalty": frequency_penalty,
                "max_tokens": max_tokens,
                "messages": messages,
                "presence_penalty": presence_penalty,
                "response_format": response_format,
                "seed": seed,
                "stop": stop,
                "stream": stream_parameter,
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_p": top_p,
            }
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_model_get_chat_completions_request(
            unknown_parameters=unknown_parameters,
            model_deployment=model_deployment,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.ChatCompletions, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    def get_embeddings(
        self,
        body: JSON,
        *,
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :param body: Required.
        :type body: JSON
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "input": [
                        "str"  # Input texts to get embeddings for, encoded as a an array of
                          strings. Required.
                    ],
                    "input_type": "str"  # Optional. Specifies the input type to use for
                      embedding search. Known values are: "text", "query", and "document".
                }

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "prompt_tokens": 0,  # Number of tokens sent in the original request.
                          Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """

    @overload
    def get_embeddings(
        self,
        *,
        input: List[str],
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        content_type: str = "application/json",
        input_type: Optional[Union[str, _models.EmbeddingInputType]] = None,
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :keyword input: Input texts to get embeddings for, encoded as a an array of strings. Required.
        :paramtype input: list[str]
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword input_type: Specifies the input type to use for embedding search. Known values are:
         "text", "query", and "document". Default value is None.
        :paramtype input_type: str or ~azure.ai.inference.models.EmbeddingInputType
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "prompt_tokens": 0,  # Number of tokens sent in the original request.
                          Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """

    @overload
    def get_embeddings(
        self,
        body: IO[bytes],
        *,
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :param body: Required.
        :type body: IO[bytes]
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "prompt_tokens": 0,  # Number of tokens sent in the original request.
                          Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """

    @distributed_trace
    def get_embeddings(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        input: List[str] = _Unset,
        unknown_parameters: Optional[Union[str, _models.UnknownParameters]] = None,
        model_deployment: Optional[str] = None,
        input_type: Optional[Union[str, _models.EmbeddingInputType]] = None,
        **kwargs: Any
    ) -> _models.EmbeddingsResult:
        # pylint: disable=line-too-long
        """Return the embeddings for a given prompt.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword input: Input texts to get embeddings for, encoded as a an array of strings. Required.
        :paramtype input: list[str]
        :keyword unknown_parameters: Controls what happens if unknown parameters are passed as extra
         properties in the request payload. Known values are: "error", "ignore", and "allow". Default
         value is None.
        :paramtype unknown_parameters: str or ~azure.ai.inference.models.UnknownParameters
        :keyword model_deployment: Name of the deployment to which you would like to route the request.
         Relevant only to Model-as-a-Platform (MaaP) deployments.
         Typically used when you want to target a test environment instead of production environment.
         Default value is None.
        :paramtype model_deployment: str
        :keyword input_type: Specifies the input type to use for embedding search. Known values are:
         "text", "query", and "document". Default value is None.
        :paramtype input_type: str or ~azure.ai.inference.models.EmbeddingInputType
        :return: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.EmbeddingsResult
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "input": [
                        "str"  # Input texts to get embeddings for, encoded as a an array of
                          strings. Required.
                    ],
                    "input_type": "str"  # Optional. Specifies the input type to use for
                      embedding search. Known values are: "text", "query", and "document".
                }

                # response body for status code(s): 200
                response == {
                    "data": [
                        {
                            "embedding": [
                                0.0  # List of embeddings value for the input prompt.
                                  These represent a measurement of the vector-based relatedness of the
                                  provided input. Required.
                            ],
                            "index": 0,  # Index of the prompt to which the EmbeddingItem
                              corresponds. Required.
                            "object": "str"  # The object type of this embeddings item.
                              Will always be ``embedding``. Required.
                        }
                    ],
                    "id": "str",  # Unique identifier for the embeddings result. Required.
                    "model": "str",  # The model ID used to generate this result. Required.
                    "object": "str",  # The object type of the embeddings result. Will always be
                      ``list``. Required.
                    "usage": {
                        "prompt_tokens": 0,  # Number of tokens sent in the original request.
                          Required.
                        "total_tokens": 0  # Total number of tokens transacted in this
                          request/response. Required.
                    }
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.EmbeddingsResult] = kwargs.pop("cls", None)

        if body is _Unset:
            if input is _Unset:
                raise TypeError("missing required argument: input")
            body = {"input": input, "input_type": input_type}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_model_get_embeddings_request(
            unknown_parameters=unknown_parameters,
            model_deployment=model_deployment,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.EmbeddingsResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
